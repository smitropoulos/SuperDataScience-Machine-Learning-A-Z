# Multiple Linear Regression
# Importing the dataset
dataset = read.csv('50_Startups.csv')
# Encoding categorical data
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Multiple Linear Regression
# Importing the dataset
dataset = read.csv('50_Startups.csv')
# Encoding categorical data
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
regressor = lm(formula = Profit ~ .,
data = training_set)
regressor = lm(formula = Profit ~.,
data = training_set)
View(regressor)
summary(regressor)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
View(test_set)
y_pred
View(test_set)
View(dataset)
regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = training_set)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,
data = training_set)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = dataset)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,
data = dataset)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,
data = dataset)
regressor = lm(formula = Profit ~ R.D.Spend + Marketing.Spend,
data = dataset)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend,
data = dataset)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend,
data = dataset)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend + Marketing.Spend,
data = dataset)
summary(regressor)
# Fitting Multiple Linear Regression to the Training set Using Backward Compatibility.
regressor = lm(formula = Profit ~ R.D.Spend,
data = dataset)
summary(regressor)
y_pred
View(dataset)
View(test_set)
setwd("~/Dropbox/Programming/Python/SuperDataScience/Machine Learning A-Z/Part 2 - Regression/Section 6 - Polynomial Regression")
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
data = dataset)
lin_reg
summary(lin_reg)
t=range(10)
range(10,100) )
range(10,100)
range(10:100)
range(100,finite=FALSE)
range(100,finite=TRUE)
range(1,100,finite=TRUE)
range(1,100,finite=FALSE)
range(1:100)
1:100
c(1:4)
c=(1:4)
for (exponent in c(1:5) ){
print(c)
}
for (exponent in c(1:5) ){
print(exponent)
}
for (exponent in c(1:5) ){
dataset$levelexponent = dataset$Level^exponent
}
View(dataset)
dataset$levelexponent = dataset$Level^exponent
for (exponent in c(1:5) ){
dataset$levelexponent = dataset$Level^exponent
}
for (exponent in c(1:5) ){
dataset$level(exponent) = dataset$Level^exponent
}
for (exponent in c(1:5) ){
dataset$level(exponent) = dataset$Level^exponent
}
dataset$levelexponent = dataset$Level^exponent
for (exponent in c(1:5) ){
dataset$levelexponent = dataset$Level^exponent
}
for (exponent in c(1:5) ){
dataset$levelexponent = dataset$Level^exponent
}
exponent
View(dataset)
for (exponent in c(1:5) ){
paste(dataset$level,exponent) = dataset$Level^exponent
}
assign(dataset$level,exponent) = dataset$Level^exponent
paste(exponent)
assign(paste("dataset$level",exponent,sep="")) = dataset$Level^exponent
exponent=2
assign(paste("dataset$level",exponent,sep="")) = dataset$Level^exponent
paste(dataset$level3) = dataset$Level^exponent
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
# Polynomial Regression
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Splitting the dataset into the Training set and Test set is not necessary since we have so little samples.
# Feature Scaling is not needed in this case since we have little independet variables (1)
# Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
data = dataset)
# Fitting Polynomial Regression to the dataset
#I suppose looping would be a way to get rid of this...
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
x <- 1:10
x=1:1-
x=1:10
poly_x <- poly(x, degree = 3)
View(poly_x)
poly_x[,1:3]
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
datasettest = dataset
x<-datasettest[2]
poly_x <- (x, degree=3)
View(x)
poly_x <- (x, degree=3)
poly_x <- poly(x, degree=3)
poly_x <- poly(x, degree=55)
poly_x <- poly(x, degree=2)
poly_x <- poly([1:10], degree=2)
poly_x <- poly(datasettest[1], degree=2)
View(datasettest)
View(x)
View(x)
sprintf("dataset$level%d"exponent) = dataset$Level^exponent
sprintf("dataset$level%d",exponent) = dataset$Level^exponent
for (exponent in c(1:5) ){
sprintf("dataset$level%d",exponent) = dataset$Level^exponent
}
sprintf("dataset$level%s",exponent) = dataset$Level^exponent
sprintf("dataset$level%s",exponent)
exponent=2
sprintf("dataset$level%s",exponent)
sprintf("dataset$level%s",exponent) = dataset$Level^exponent
for (exponent in c(1:5) ){
}
exponent=0
for (exponent;exponent<5;exponent+=1 ){
for (exponent in 1:5 ){
sprintf("dataset$level%d",exponent) = dataset$Level^exponent
}
for (exponent in 1:5 ){
}
exponent = 1
repeat{
sprintf("dataset$level%d",exponent) = dataset$Level^exponent
exponent++
if(exponent == 5){
break
}
}
exponent = 1
sprintf("dataset$level%d",exponent) = dataset$Level^exponent
sprintf("dataset$level%d",exponent) = sprintf("dataset$Level^exponent",exponent)
sprintf("dataset$level%d",exponent) = sprintf("dataset$Level^%d",exponent)
sprintf("dataset$Level^%d",exponent)
sprintf("dataset$level%d",exponent)
poly_reg = lm(formula = Salary ~ .,
data = dataset)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Linear Regression)') +
xlab('Level') +
ylab('Salary')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Linear Regression)') +
xlab('Level') +
ylab('Salary')
# Polynomial Regression
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Splitting the dataset into the Training set and Test set is not necessary since we have so little samples.
# Feature Scaling is not needed in this case since we have little independet variables (1)
# Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
data = dataset)
# Fitting Polynomial Regression to the dataset
#I suppose looping would be a way to get rid of this...
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
# exponent = 1
# repeat{
#   sprintf("dataset$level%d",exponent) = sprintf("dataset$Level^%d",exponent)
#   exponent++
#     if(exponent == 5){
#       break
#     }
# }
poly_reg = lm(formula = Salary ~ .,
data = dataset)
# Visualising the Linear Regression results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Linear Regression)') +
xlab('Level') +
ylab('Salary')
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(poly_reg,
newdata = data.frame(Level = x_grid,
Level2 = x_grid^2,
Level3 = x_grid^3,
Level4 = x_grid^4))),
colour = 'blue') +
ggtitle('Truth or Bluff (Polynomial Regression)') +
xlab('Level') +
ylab('Salary')
# Visualising the Polynomial Regression results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Polynomial Regression)') +
xlab('Level') +
ylab('Salary')
# Predicting a new result with Linear Regression
predict(lin_reg, data.frame(Level = 6.5))
# Predicting a new result with Polynomial Regression
predict(poly_reg, data.frame(Level = 6.5,
Level2 = 6.5^2,
Level3 = 6.5^3,
Level4 = 6.5^4))
# Polynomial Regression
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Splitting the dataset into the Training set and Test set is not necessary since we have so little samples.
# Feature Scaling is not needed in this case since we have little independet variables (1)
# Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
data = dataset)
# Fitting Polynomial Regression to the dataset
#I suppose looping would be a way to get rid of this...
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
# exponent = 1
# repeat{
#   sprintf("dataset$level%d",exponent) = sprintf("dataset$Level^%d",exponent)
#   exponent++
#     if(exponent == 5){
#       break
#     }
# }
poly_reg = lm(formula = Salary ~ .,
data = dataset)
# Visualising the Linear Regression results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Linear Regression)') +
xlab('Level') +
ylab('Salary')
# Visualising the Polynomial Regression results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Polynomial Regression)') +
xlab('Level') +
ylab('Salary')
# Visualising the Regression Model results (for higher resolution and smoother curve)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(poly_reg,
newdata = data.frame(Level = x_grid,
Level2 = x_grid^2,
Level3 = x_grid^3,
Level4 = x_grid^4))),
colour = 'blue') +
ggtitle('Truth or Bluff (Polynomial Regression)') +
xlab('Level') +
ylab('Salary')
# Predicting a new result with Linear Regression
predict(lin_reg, data.frame(Level = 6.5))
# Predicting a new result with Polynomial Regression
predict(poly_reg, data.frame(Level = 6.5,
Level2 = 6.5^2,
Level3 = 6.5^3,
Level4 = 6.5^4))
# SVR
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Splitting the dataset into the Training set and Test set
# # install.packages('caTools')
# library(caTools)
# set.seed(123)
# split = sample.split(dataset$Salary, SplitRatio = 2/3)
# training_set = subset(dataset, split == TRUE)
# test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting SVR to the dataset
# install.packages('e1071')
library(e1071)
regressor = svm(formula = Salary ~ .,
data = dataset,
type = 'eps-regression',
kernel = 'radial')
# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))
# Visualising the SVR results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
# Visualising the SVR results (for higher resolution and smoother curve)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
# SVR
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Splitting the dataset into the Training set and Test set
# # install.packages('caTools')
# library(caTools)
# set.seed(123)
# split = sample.split(dataset$Salary, SplitRatio = 2/3)
# training_set = subset(dataset, split == TRUE)
# test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting SVR to the dataset
# install.packages('e1071')
library(e1071)
regressor = svm(formula = Salary ~ .,
data = dataset,
type = 'eps-regression',
kernel = 'radial')
# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))
# Visualising the SVR results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
# Visualising the SVR results (for higher resolution and smoother curve)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
# SVR
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
library(e1071)
install.packages("e1071")
# SVR
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
# Splitting the dataset into the Training set and Test set
# # install.packages('caTools')
# library(caTools)
# set.seed(123)
# split = sample.split(dataset$Salary, SplitRatio = 2/3)
# training_set = subset(dataset, split == TRUE)
# test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting SVR to the dataset
# install.packages('e1071')
library(e1071)
regressor = svm(formula = Salary ~ .,
data = dataset,
type = 'eps-regression',
kernel = 'radial')
# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))
# Visualising the SVR results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
# Visualising the SVR results (for higher resolution and smoother curve)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (SVR)') +
xlab('Level') +
ylab('Salary')
install.packages(c("BH", "DBI", "DRR", "MASS", "Matrix", "RCurl", "RJDBC", "Rcpp", "TTR", "bindr", "bindrcpp", "blob", "broom", "callr", "caret", "cluster", "config", "curl", "dbplyr", "ddalpha", "digest", "forcats", "glmnet", "haven", "hexbin", "hms", "htmlwidgets", "httpuv", "knitr", "lava", "lubridate", "maps", "mgcv", "mongolite", "nlme", "odbc", "openssl", "packrat", "pbdZMQ", "pillar", "plogr", "profvis", "pryr", "psych", "randomForest", "recipes", "reprex", "rlang", "rmarkdown", "rpart", "rprojroot", "rsconnect", "selectr", "sfsmisc", "sparklyr", "stringi", "stringr", "tibble", "tidyr", "tidyselect", "timeDate", "utf8", "viridisLite", "withr", "xml2", "xts", "yaml", "zoo"))
install.packages(c("BH", "DBI", "DRR", "MASS", "Matrix", "RCurl", "RJDBC", "Rcpp", "TTR", "bindr", "bindrcpp", "blob", "broom", "callr", "caret", "cluster", "config", "curl", "dbplyr", "ddalpha", "digest", "forcats", "glmnet", "haven", "hexbin", "hms", "htmlwidgets", "httpuv", "knitr", "lava", "lubridate", "maps", "mgcv", "mongolite", "nlme", "odbc", "openssl", "packrat", "pbdZMQ", "pillar", "plogr", "profvis", "pryr", "psych", "randomForest", "recipes", "reprex", "rlang", "rmarkdown", "rpart", "rprojroot", "rsconnect", "selectr", "sfsmisc", "sparklyr", "stringi", "stringr", "tibble", "tidyr", "tidyselect", "timeDate", "utf8", "viridisLite", "withr", "xml2", "xts", "yaml", "zoo"))
install.packages(c("BH", "DBI", "DRR", "MASS", "Matrix", "RCurl", "RJDBC", "Rcpp", "TTR", "bindr", "bindrcpp", "blob", "broom", "callr", "caret", "cluster", "config", "curl", "dbplyr", "ddalpha", "digest", "forcats", "glmnet", "haven", "hexbin", "hms", "htmlwidgets", "httpuv", "knitr", "lava", "lubridate", "maps", "mgcv", "mongolite", "nlme", "odbc", "openssl", "packrat", "pbdZMQ", "pillar", "plogr", "profvis", "pryr", "psych", "randomForest", "recipes", "reprex", "rlang", "rmarkdown", "rpart", "rprojroot", "rsconnect", "selectr", "sfsmisc", "sparklyr", "stringi", "stringr", "tibble", "tidyr", "tidyselect", "timeDate", "utf8", "viridisLite", "withr", "xml2", "xts", "yaml", "zoo"))
install.packages("e1071", dependencies = FALSE)
